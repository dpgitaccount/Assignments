{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134e22c-19ac-4dba-a23a-5dd24542b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1\n",
    "ANS-\n",
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to search for the particular data required by following the links across the internet. The scraper, on the other hand, is a specific tool created to extract data from the website. The design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly and accurately extract the data.\n",
    "\n",
    "Web scraping is used in a variety of digital businesses that rely on data harvesting. Legitimate use cases include: Search engine bots crawling a site, analyzing its content and then ranking it. Price comparison sites deploying bots to auto-fetch prices and product descriptions for allied seller websites.\n",
    "\n",
    "1. Price Monitoring\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "2. Market Research\n",
    "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "3. News Monitoring\n",
    "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa386786-a1a0-46ac-8985-a4edff0ad25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2.\n",
    "ANS- The given methods are used for web scrapping\n",
    "1.Human Copy-and-Paste\n",
    "Manually copying and pasting data from a web page into a text file or spreadsheet is the most basic form of web scraping. Even the best web-scraping technology cannot always replace a human’s manual examination and copy-and-paste, and this may be the only viable option when the websites for scraping explicitly prohibit machine automation.\n",
    "Text Pattern Matching\n",
    "The UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
    "\n",
    "2,HTTP Programming\n",
    "Static and dynamic web pages can be retrieved by using socket programming to send HTTP requests to a remote web server.\n",
    "\n",
    "3.HTML Parsing\n",
    "Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form.\n",
    "Wrapper generation algorithms assume that the input pages of a wrapper induction system follow a common template and can be identified using a URL common scheme. [2] Furthermore, semi-structured data query languages such as XQuery and HTQL can be used to parse HTML pages as well as retrieve and transform page content.\n",
    "\n",
    "4.DOM Parsing\n",
    "More information: Object Model for Documents, Programs can retrieve dynamic content generated by client-side scripts by embedding a full-fledged web browser, such as Internet Explorer or the Mozilla browser control. These browser controls also parse web pages into a DOM tree, which programs can use to retrieve portions of the pages. The resulting DOM tree can be parsed using languages such as Xpath.\n",
    "\n",
    "5.Vertical Aggregation\n",
    "Several companies have created vertically specific harvesting platforms. These platforms generate and monitor a plethora of “bots” for specific verticals with no “man in the loop” (direct human involvement) and no work related to a specific target site. The preparation entails creating a knowledge base for the entire vertical, after which the platform will create the bots automatically.\n",
    "The robustness of the platform is measured by the quality of the information it retrieves (typically the number of fields) and its scalability (how quickly it can scale up to hundreds or thousands of sites). This scalability is primarily used to target the Long Tail of sites that common aggregators find too difficult or time-consuming to harvest content from.\n",
    "\n",
    "6.Semantic Annotation Recognizing\n",
    "The scraped pages may include metadata, semantic markups, and annotations that can be used to locate specific data snippets. This technique can be viewed as a subset of DOM parsing if the annotations are embedded in the pages, as Microformat does. In another case, the annotations are stored and managed separately from the web pages, so scrapers can retrieve data schema and instructions from this layer before scraping the pages.\n",
    "\n",
    "7.Computer Vision Web-Page Analysis\n",
    "There are efforts using machine learning and computer vision to identify and extract information from web pages by visually interpreting pages as a human would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369ac12-78f5-4fd5-99fa-393f62254dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3.\n",
    "ANS-\n",
    "Beautiful Soup is a Python library used for web scraping purposes. It provides a set of tools for parsing HTML and XML documents, and navigating through their contents. Beautiful Soup allows users to extract data from web pages by providing a simple API for traversing the HTML tree and accessing specific elements.\n",
    "\n",
    "Beautiful Soup is commonly used in data science and web development to extract data from websites. It can be used to scrape text, links, images, and other content from HTML and XML files, and to transform and clean up the data as needed.\n",
    "\n",
    "Some of the key features of Beautiful Soup include:-\n",
    "\n",
    "Easy-to-use API for parsing and navigating HTML and XML documents\n",
    "Robust handling of poorly formatted or malformed HTML\n",
    "Support for multiple parsing libraries, including lxml, html5lib, and Python’s built-in parser\n",
    "Comprehensive documentation and community support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acedd7d9-3647-4a17-9be6-eac4555ed34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4.\n",
    "ANS-\n",
    "Flask is a lightweight and easy-to-use web framework in Python, which makes it a popular choice for building web applications and APIs. Flask is often used in web scraping projects because it allows developers to quickly build a web interface for their scraping code, making it easier to interact with and visualize the data that is being extracted.\n",
    "\n",
    "In a web scraping project, Flask can be used to build a web application that provides a simple user interface for initiating and managing the scraping process. For example, the Flask application can have a form where users can enter a URL to be scraped, select the type of data to extract, and specify any other parameters.\n",
    "Once the user submits the form, Flask can initiate the scraping process and return the results in a format that can be easily displayed in the web interface. Flask can also be used to handle errors and exceptions that may arise during the scraping process, and provide feedback to the user on the progress of the scraping operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a92c7-bdfa-4260-b85b-99da5d46443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5.\n",
    "ANS- Given services are used in the project\n",
    "1.beautifulsoup4\n",
    "2.bs4\n",
    "3.certifi\n",
    "4.chardet\n",
    "5.click\n",
    "6.Flask\n",
    "7.Flask-Cors\n",
    "8.gunicorn\n",
    "9.idna\n",
    "10.itsdangerous\n",
    "11.Jinja2\n",
    "12.MarkupSafe\n",
    "13.requests\n",
    "14.six\n",
    "15.soupsieve\n",
    "16.urllib3\n",
    "17.Werkzeug\n",
    "18.pymongo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
